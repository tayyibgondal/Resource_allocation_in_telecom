{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "xEKHuuaqRHjz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "Elh_PcRfQqYW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 100 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH8xWIJAQw8n",
        "outputId": "50b1a767-aa6f-4986-fb10-12be176aade1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Microsoft/Phi-3.5-mini-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG9CF1iaRKmU",
        "outputId": "b9a6168a-e768-4d5c-a8d5-6d6d01372078"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.0: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"][0]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    contexts      = examples[\"context\"]\n",
        "    sample_indices = examples[\"sample_index\"]\n",
        "    texts = []\n",
        "    # for instruction, input, context, output in zip(instructions, inputs, contexts, outputs):\n",
        "    #   # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "    #   text = alpaca_prompt.format(instruction, input, context, output) + EOS_TOKEN\n",
        "    #   texts.append(text)\n",
        "    # return { \"text\" : texts, }\n",
        "\n",
        "    for input_val, output_val in zip(inputs, outputs):\n",
        "        # Format string with both input and output, and ensure EOS_TOKEN is added\n",
        "        text = f\"#INPUT \\n{input_val} \\n\" + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts, \"sample_index\" : sample_indices, }"
      ],
      "metadata": {
        "id": "A5jCDH_zRXO1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the validation dataset with retrieved context\n",
        "from datasets import load_dataset\n",
        "augmented_dataset = load_dataset(\"aamina/channel_gains_vs_tx_powers_ee_augmented_with_300_examples_context\", split=\"validation[:1000]\")"
      ],
      "metadata": {
        "id": "EdmBXwWDRdZJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_dataset = augmented_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "_Y1yB0wcSf7K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Extract Required Result from LLM Responses\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_response(input_response):\n",
        "\n",
        "  # Find all matches in the input string\n",
        "  digits = re.findall(r'\\d', input_response)\n",
        "\n",
        "  if digits:\n",
        "    return(f\"{digits[0]}, {digits[1]}\")\n",
        "  else:\n",
        "    return (\"No match found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DFd3XSlSSnA7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sample(entry):\n",
        "\n",
        "    # Split the entry into input and output parts\n",
        "    input_part = entry.split(\"then\")[0].strip()\n",
        "    output_part = \"then \" + entry.split(\"then\")[1].strip()\n",
        "\n",
        "    # Format the new output\n",
        "    formatted_sample = (f\"INPUT\\n{input_part}\\nOUTPUT\\n{output_part}\\n\")\n",
        "\n",
        "    return formatted_sample"
      ],
      "metadata": {
        "id": "_WJ1AlF6TmFY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import gc\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Set up for measuring inference time\n",
        "num_inferences = 100\n",
        "total_time = 0\n",
        "\n",
        "\n",
        "results = []\n",
        "for i in tqdm(range(num_inferences), desc=\"Running Inference\"):\n",
        "  query = augmented_dataset[i]\n",
        "  # Prompt Engineering\n",
        "  prompt_string = query[\"instruction\"] + \"\\n\"\n",
        "  context = query[\"context\"]\n",
        "  context_list = context.split('\\n')\n",
        "  formatted_context = \"\"\n",
        "  for sample in context_list:\n",
        "    formatted_context = formatted_context + '\\n' + format_sample(sample)\n",
        "  # print(formatted_context)\n",
        "  # print(query[\"input\"].rstrip(','))\n",
        "\n",
        "  prompt_string = prompt_string + formatted_context + \"\\n\" + \"Give values of B close to the examples provided above. Your answer should be bigger than 10.\\n#INPUT:\\n\" + query[\"input\"] +  \"\\n#OUTPUT\\nthen B is ?\\nGive two values bigger than 10.\"\n",
        "\n",
        "  # print(prompt_string, '\\n')\n",
        "  # print(query[\"input\"])\n",
        "\n",
        "  # Tokenizing the prompt to feed into Model\n",
        "  inputs = tokenizer(prompt_string + tokenizer.eos_token, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = inputs['input_ids']  # Extract input_ids from your provided tensor\n",
        "  attention_mask = inputs['attention_mask']\n",
        "\n",
        "  # print(inputs)\n",
        "  # print(attention_mask)\n",
        "  # Prompting the Model\n",
        "\n",
        "  # Measure inference time\n",
        "  start_time = time.time()\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=100)\n",
        "  end_time = time.time()\n",
        "\n",
        "  inference_time = end_time - start_time\n",
        "  total_time += inference_time\n",
        "  print(f\"Inference {i+1} took {inference_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# Calculate and print the average time\n",
        "average_time = total_time / num_inferences\n",
        "print(f\"\\nAverage inference time over {num_inferences} runs: {average_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2l5L45SvdF",
        "outputId": "18f6b78a-f48b-4a4d-ed75-67e77d7dfe12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Inference:   1%|          | 1/100 [00:20<33:06, 20.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 1 took 20.0392 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   2%|▏         | 2/100 [00:35<27:59, 17.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 2 took 15.0686 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   3%|▎         | 3/100 [00:50<26:02, 16.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 3 took 14.8678 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   4%|▍         | 4/100 [01:05<25:30, 15.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 4 took 15.6809 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   5%|▌         | 5/100 [01:21<25:00, 15.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 5 took 15.5201 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   6%|▌         | 6/100 [01:36<24:30, 15.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 6 took 15.3229 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   7%|▋         | 7/100 [01:51<24:05, 15.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 7 took 15.2901 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   8%|▊         | 8/100 [02:07<23:41, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 8 took 15.2254 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:   9%|▉         | 9/100 [02:22<23:29, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 9 took 15.5534 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  10%|█         | 10/100 [02:38<23:11, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 10 took 15.3995 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  11%|█         | 11/100 [02:53<22:57, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 11 took 15.4977 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  12%|█▏        | 12/100 [03:09<22:42, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 12 took 15.4619 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  13%|█▎        | 13/100 [03:24<22:19, 15.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 13 took 15.1615 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  14%|█▍        | 14/100 [03:39<22:08, 15.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 14 took 15.5418 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  15%|█▌        | 15/100 [03:55<21:49, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 15 took 15.3162 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  16%|█▌        | 16/100 [04:10<21:31, 15.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 16 took 15.2827 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  17%|█▋        | 17/100 [04:25<21:17, 15.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 17 took 15.3877 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  18%|█▊        | 18/100 [04:41<21:12, 15.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 18 took 15.8119 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  19%|█▉        | 19/100 [04:58<21:28, 15.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 19 took 16.7566 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  20%|██        | 20/100 [05:13<20:52, 15.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 20 took 15.0373 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  21%|██        | 21/100 [05:28<20:20, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 21 took 14.9473 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  22%|██▏       | 22/100 [05:44<20:03, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 22 took 15.3710 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  23%|██▎       | 23/100 [05:59<19:44, 15.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 23 took 15.2526 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  24%|██▍       | 24/100 [06:15<19:53, 15.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 24 took 16.4187 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  25%|██▌       | 25/100 [06:31<19:33, 15.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 25 took 15.4807 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  26%|██▌       | 26/100 [06:46<19:14, 15.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 26 took 15.4929 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  27%|██▋       | 27/100 [07:02<18:54, 15.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 27 took 15.3781 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  28%|██▊       | 28/100 [07:17<18:38, 15.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 28 took 15.5102 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  29%|██▉       | 29/100 [07:33<18:22, 15.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 29 took 15.4788 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  30%|███       | 30/100 [07:48<18:06, 15.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 30 took 15.4698 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  31%|███       | 31/100 [08:04<17:47, 15.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 31 took 15.3449 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  32%|███▏      | 32/100 [08:19<17:30, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 32 took 15.3986 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  33%|███▎      | 33/100 [08:35<17:17, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 33 took 15.5321 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  34%|███▍      | 34/100 [08:50<16:59, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 34 took 15.3476 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  35%|███▌      | 35/100 [09:05<16:41, 15.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 35 took 15.2625 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  36%|███▌      | 36/100 [09:21<16:24, 15.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 36 took 15.3267 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  37%|███▋      | 37/100 [09:36<16:10, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 37 took 15.4319 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  38%|███▊      | 38/100 [09:51<15:56, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 38 took 15.4538 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  39%|███▉      | 39/100 [10:07<15:40, 15.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 39 took 15.3719 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  40%|████      | 40/100 [10:22<15:19, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 40 took 15.0700 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  41%|████      | 41/100 [10:37<15:04, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 41 took 15.3113 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  42%|████▏     | 42/100 [10:52<14:45, 15.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 42 took 15.1193 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  43%|████▎     | 43/100 [11:08<14:27, 15.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 43 took 15.0928 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  44%|████▍     | 44/100 [11:23<14:15, 15.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 44 took 15.3770 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  45%|████▌     | 45/100 [11:38<14:03, 15.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 45 took 15.4855 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  46%|████▌     | 46/100 [11:54<13:49, 15.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 46 took 15.3889 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  47%|████▋     | 47/100 [12:09<13:36, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 47 took 15.5001 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  48%|████▊     | 48/100 [12:25<13:23, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 48 took 15.5418 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  49%|████▉     | 49/100 [12:40<13:05, 15.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 49 took 15.2531 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  50%|█████     | 50/100 [12:56<12:50, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 50 took 15.4021 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  51%|█████     | 51/100 [13:11<12:29, 15.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 51 took 15.0096 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  52%|█████▏    | 52/100 [13:26<12:17, 15.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 52 took 15.4635 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  53%|█████▎    | 53/100 [13:42<12:03, 15.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 53 took 15.4314 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  54%|█████▍    | 54/100 [13:57<11:51, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 54 took 15.6253 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  55%|█████▌    | 55/100 [14:13<11:34, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 55 took 15.3259 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  56%|█████▌    | 56/100 [14:28<11:16, 15.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 56 took 15.2003 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  57%|█████▋    | 57/100 [14:43<10:56, 15.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 57 took 15.0288 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  58%|█████▊    | 58/100 [14:58<10:43, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 58 took 15.3807 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  59%|█████▉    | 59/100 [15:14<10:28, 15.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 59 took 15.3614 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  60%|██████    | 60/100 [15:29<10:14, 15.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 60 took 15.3817 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  61%|██████    | 61/100 [15:44<09:55, 15.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 61 took 15.0809 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  62%|██████▏   | 62/100 [15:59<09:38, 15.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 62 took 15.0703 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  63%|██████▎   | 63/100 [16:15<09:25, 15.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 63 took 15.3996 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  64%|██████▍   | 64/100 [16:30<09:11, 15.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 64 took 15.3522 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  65%|██████▌   | 65/100 [16:46<08:56, 15.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 65 took 15.3890 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  66%|██████▌   | 66/100 [17:01<08:42, 15.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 66 took 15.4519 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  67%|██████▋   | 67/100 [17:16<08:28, 15.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 67 took 15.4202 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  68%|██████▊   | 68/100 [17:32<08:12, 15.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 68 took 15.3542 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  69%|██████▉   | 69/100 [17:47<07:58, 15.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 69 took 15.5310 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  70%|███████   | 70/100 [18:03<07:42, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 70 took 15.3111 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  71%|███████   | 71/100 [18:18<07:27, 15.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 71 took 15.4380 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  72%|███████▏  | 72/100 [18:34<07:12, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 72 took 15.4828 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  73%|███████▎  | 73/100 [18:49<06:58, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 73 took 15.5669 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  74%|███████▍  | 74/100 [19:05<06:43, 15.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 74 took 15.5031 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  75%|███████▌  | 75/100 [19:20<06:27, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 75 took 15.4306 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  76%|███████▌  | 76/100 [19:36<06:12, 15.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 76 took 15.5164 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  77%|███████▋  | 77/100 [19:51<05:55, 15.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 77 took 15.3782 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  78%|███████▊  | 78/100 [20:07<05:39, 15.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 78 took 15.3498 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  79%|███████▉  | 79/100 [20:22<05:25, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 79 took 15.5908 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  80%|████████  | 80/100 [20:38<05:09, 15.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 80 took 15.4713 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  81%|████████  | 81/100 [20:53<04:54, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 81 took 15.4421 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  82%|████████▏ | 82/100 [21:08<04:37, 15.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 82 took 15.3132 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  83%|████████▎ | 83/100 [21:24<04:22, 15.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 83 took 15.5142 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  84%|████████▍ | 84/100 [21:39<04:06, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 84 took 15.3275 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  85%|████████▌ | 85/100 [21:55<03:51, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 85 took 15.3312 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  86%|████████▌ | 86/100 [22:10<03:36, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 86 took 15.4574 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  87%|████████▋ | 87/100 [22:26<03:21, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 87 took 15.5775 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  88%|████████▊ | 88/100 [22:41<03:05, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 88 took 15.4852 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  89%|████████▉ | 89/100 [22:57<02:50, 15.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 89 took 15.5641 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  90%|█████████ | 90/100 [23:12<02:34, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 90 took 15.3061 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  91%|█████████ | 91/100 [23:28<02:19, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 91 took 15.4021 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  92%|█████████▏| 92/100 [23:43<02:03, 15.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 92 took 15.2929 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  93%|█████████▎| 93/100 [23:58<01:48, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 93 took 15.5164 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  94%|█████████▍| 94/100 [24:14<01:32, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 94 took 15.4664 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  95%|█████████▌| 95/100 [24:29<01:16, 15.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 95 took 15.1606 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  96%|█████████▌| 96/100 [24:45<01:01, 15.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 96 took 15.5040 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  97%|█████████▋| 97/100 [25:00<00:46, 15.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 97 took 15.4808 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  98%|█████████▊| 98/100 [25:16<00:30, 15.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 98 took 15.3416 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRunning Inference:  99%|█████████▉| 99/100 [25:30<00:15, 15.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 99 took 14.9010 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Inference: 100%|██████████| 100/100 [25:46<00:00, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference 100 took 15.3155 seconds\n",
            "\n",
            "Average inference time over 100 runs: 15.4403 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
